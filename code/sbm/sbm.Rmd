---
title: Using a Stochastic Block Model to quantify the difference between DTI and fMRI
  Connectomes
author: "Eric Bridgeford"
date: "September 29, 2017"
header-includes:
   - \usepackage{amsmath}
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setting

## Task

Given: 

+ $n$ samples of diffusion connectomes, $D = \left\{d_i\right\}_{i=1}^n$ where $d_i  = (E, V, w)$ for $|V|$ regions of interest and $w(v_i, v_j) = w_{ij} \in [0, 1]$.
+ $m$ samples of functional connectomes, $F = \left\{f_i\right\}_{i=1}^m$ where $f_i = (E, V, w)$ for $|V|$ regions of interest and $w(v_i, v_j) = w_{ij} \in [0, 1]$.
+ a partitioning of the vertices into $C_1$, the left hemisphere ROIs, and $C_2$, the right hemisphere ROIs, where $C_1 \cup C_2 = V$.

Determine if there exists a difference in the connectivity among the vertices in $C_1$ and $C_2$ within a particular modality, and between the communities $C_1$ and $C_2$, for the functional vs diffusion connectomes. 

## Statistical Model

Assume we have a random variable $A$ which can be characterized by the Stochastic Block Model with parameters $G$, $B$:

\begin{align*}
	A \sim SBM(G, B)
\end{align*}

where $G$ is a grouping of the $N$ vertices in our graph into our respective communities and $B$ represents the parameters for within and between group edge probabilities. Assuming the number of edges in our graph are binomially distributed wrt the parameter $p$, we can estimate the number of edges for each group:

\begin{align*}
  f_B(n, k | p) &= \begin{pmatrix}n \\ k\end{pmatrix}p^k (1 - p)^{n - k} \\
  \mu_{\hat{k}} &= np \\
  \sigma^2_{\hat{k}} &= np(1 - p)
\end{align*}

Assuming that $p = \frac{k}{n}$, then we can see that:

\begin{align*}
  \mu_{\hat{p}} &= \frac{k}{n} \\
  \sigma^2_{\hat{p}} &= \frac{np(1-p)}{n^2} = \frac{p(1-p)}{n}
\end{align*}

where $p$ is the probability of a given edge, $k$ are the number of connected edges, and $n$ is the number of possible edges. We can therefore define an estimator of $B$, $\hat{B}$ where connections betweeb community $C_l$ and $C_m$ can be modelled:

\begin{align*}
  \hat{B}_{lm} &= \mathcal{N}(\mu_{lm}, \sigma_{lm})
\end{align*}

where $\hat{\mu}_{lm} = \sum_{(i, j) \in E(C_l \times C_m)} A_{ij}$, and $\hat{\sigma}^2_{k} = \frac{\hat{\mu}_{l}(1 - \hat{\mu}_{l})}{\left|C_l \times C_m\right|}$.

Grouping our vertices into the left and right hemispheres, under the above model we end up with:

\begin{align*}
  
	\hat{B} &\sim \mathcal{N}\left(\mu_B, \Sigma_B\right) \\
    \hat{\mu}_B &= \begin{bmatrix}
    	p_{ll} \\ p_{rl} \\
        p_{lr} \\ p_{rr}
    \end{bmatrix} \\
    \hat{\Sigma}_B &= \begin{bmatrix}
    	\frac{p_{ll}(1 - p_{ll})}{\left(\frac{N}{2}\right)^2} & 0 & 0 & 0 \\
    	0 & \frac{p_{rl}(1 - p_{rl})}{\left(\frac{N}{2}\right)^2} & 0 & 0 \\
    	0 & 0 & \frac{p_{lr}(1 - p_{lr})}{\left(\frac{N}{2}\right)^2} & 0 \\
    	0 & 0 & 0 & \frac{p_{rr}(1 - p_{rr})}{\left(\frac{N}{2}\right)^2} \\
    \end{bmatrix}
\end{align*}

1) regrouping to only consider the ipsi-lateral vs. contra-lateral edges, we can simplify:

\begin{align*}
    \hat{\mu}_B &= \begin{bmatrix}
    	p_{i} \\
        p_{c} 
    \end{bmatrix} \\
    \hat{\Sigma}_B &= \begin{bmatrix}
    	\frac{p_{i}(1 - p_{i})}{\left(\frac{N}{2}\right)^2} & 0 \\
    	0 & \frac{p_{c}(1 - p_{c})}{\left(\frac{N}{2}\right)^2}
    \end{bmatrix}
\end{align*}

where $p_i$ represents the probability of a ipsi-lateral connection, and $p_c$ the probability of a contra-lateral connection. Then given the connectome megamean as an adjacency matrix $M \in \left\{0, 1\right\}^{N \times N}$ with $N$ vertices, we can compute $p_i$ and $p_c$ as follows:

\begin{align*}
	I &= \left\{(i, j): \textrm{vertex $i$ and vertex $j$ are in the same hemisphere}\right\} \\
	C &= \left\{(i, j): \textrm{vertex $i$ and vertex $j$ are not in the same hemisphere}\right\} \\
    p_i &= \frac{1}{|I|} \sum_{(i, j) \in I} M_{ij} \\
    p_c &= \frac{1}{|C|} \sum_{(i, j) \in C} M_{ij}
\end{align*}

2) Given the following random variables $D, F$:

\begin{align*}
    D &\sim \mathcal{N}\left(\mu_D, \Sigma_D\right) \\
    \mu_D &= \begin{bmatrix}D_{11} \\ D_{12} \\ D_{22}\end{bmatrix} \\
    \Sigma_D &= \begin{bmatrix}
    \frac{D_{11}(1 - D_{11})}{\begin{pmatrix}n \\ 2\end{pmatrix}} & 0 & 0 \\
    0 & \frac{D_{12}(1 - D_{12})}{\left(\frac{n}{2}\right)^2} & 0 \\
    0 & 0 & \frac{D_{22}(1 - D_{22})}{\begin{pmatrix}n \\ 2\end{pmatrix}}
    \end{bmatrix}
\end{align*}

\begin{align*}
    F &\sim \mathcal{N}\left(\mu_F, \Sigma_F\right) \\
    \mu_F &= \begin{bmatrix}F_{11} \\ F_{12} \\ F_{22}\end{bmatrix} \\
    \Sigma_F &= \begin{bmatrix}
    \frac{F_{11}(1 - F_{11})}{\begin{pmatrix}n \\ 2\end{pmatrix}} & 0 & 0 \\
    0 & \frac{F_{12}(1 - F_{12})}{\left(\frac{n}{2}\right)^2} & 0 \\
    0 & 0 & \frac{F_{22}(1 - F_{22})}{\begin{pmatrix}n \\ 2\end{pmatrix}}
    \end{bmatrix}
\end{align*}

## Statistical Goal

1) with $H_0: p_I <= p_C$, $H_A: p_I > p_C$, determine:

\begin{align*}
  \mathbb{P}(\textrm{reject $H_0$ in favor of $H_A$ | $H_0$ is true})
\end{align*}

That is, determine the probability of incorrectly rejecting the null hypothesis that the connectivity across hemisphere is less than the connectivity within hemisphere. 

2) with $H_0: \mu_D = \mu_F$, $H_A: \mu_D \neq \mu_F$, determine:

\begin{align*}
  \mathbb{P}(\textrm{reject $H_0$ in favor of $H_A$ | $H_0$ is true})
\end{align*}

That is, determine the probability that the connectivity in the diffusion connectomes differs from the connectivity in the functional connectomes under a weighted SBM framework.

## Test Statistic

1) [Welch's T-Test](https://en.wikipedia.org/wiki/Welch%27s_t-test) for testing whether populations have equal means given that they have different variances in the univariate case.

\begin{align*}
    T = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
\end{align*}

Since our random variables are all assumed to be independent, we can assume that $s_1 = \Sigma_{11}$, and $s_2 = \Sigma_{22}$.

and the degrees of freedom can be calculated as follows:

\begin{align*}
    \nu &= \frac{\left(\frac{s_I^2}{n_1} + \frac{s_C^2}{n_2}\right)^2}{\frac{s_1^4}{n_1^2 \nu_1} + \frac{s_2^4}{n_2^2\nu_2}}
\end{align*}

where the standard deviation $s_j = \sqrt{\frac{p_j(1 - p_j)}{\left(\frac{N}{2}\right)^2}}$

where $\nu_1 = n_1 - 1, \; \nu_2 = n_2 - 1$.

We can then use a one-sided test given $T, \nu$ to get a $p-$ value.

2)

## P-Value

1) We can compute a p-value of falsely rejecting the null hypothesis by simply finding the area:

\begin{align*}
    p = \int_{-T_{observed}}^{\infty}p(x, df) dx = 1 - \int_{-\infty}^{T_{observed}} p(x, df) dx
\end{align*}

where $p(x, df)$ is the pdf for the $T$ distribution with degrees of freedom $df$.

## Statistical Power

1) The statistical power can be computed as the inverse of the probability of making a Type II ($\beta$) error. A type II error can be defined as follows:

\begin{align*}
    \beta = \mathbb{P}(\textrm{reject $H_A$ in favor of $H_0$ | $H_A$ is true}) = \mathbb{P}(T_{observed} > T_{critical})
\end{align*}

where $T_{critical}$ is the test-statistic at the given level of significance $\alpha$ specified by our test. To compute the power, we will compute the rejection cutoff for the test-statistic, and then simulate data under the alternative hypothesis, and see how many times we would reject the null hypothesis in our simulated data. In pseudo-code:

```{r, eval=FALSE}
Compute_Power(n, means, sds, sig=.95):
  cutoff = T_{dist}(sig, df=n-2)
  tstat = []
  for i in 1:n
    snull = random_normal(n, means[1], sds[1])
    salt = random_normal(n, means[2], sds[2])
    tstat[i] = welch_ttest(snull, salt)$statistic
  end
  return(sum(ts > cutoff)/n)
```

# Simulations

## Simulated Data

### Consistency of Estimators for $\hat{p}$

Here, we will verify that our estimators of $\hat{p}$ are correct, that is, that we can accurately estimate $\mu_{\hat{p}}$ and $\sigma^2_{\hat{p}}$ given binomially distributed edges:

```{r}
require(ggplot2)
library(latex2exp)
model.var = function(p, n) {
  p*(1 - p)/n
}
model.params = function(dat) {
  mu = sum(dat)/length(dat)
  var = model.var(mu)
  return(list(mu = mu, sigma = sqrt(var)))
}

ns = round(10^seq(1, log10(1225), length=10))
ps = seq(0, 1, length=11)
ndat = length(ns)*length(ps)
empty_ar = array(NaN, dim=c(ndat))
results = data.frame(n = empty_ar, p = empty_ar, mu = empty_ar, var = empty_ar)
counter = 1
nsim = 10
for (n in ns) {
  for (p in ps) {
    v_ar = array(NaN, dim=c(nsim))
    m_ar = array(NaN, dim=c(nsim))
    for (i in 1:nsim) {
      pemp = replicate(n, {
        dat = rbinom(n = n, p = p, size=1)
        phat = sum(dat)/length(dat)
        })
      m_ar[i] = abs(mean(pemp) - p)
      v_ar[i] = abs(var(pemp) - model.var(p, n))
    }
    results[counter,] = data.frame(n = n, p = p, mu = mean(m_ar),
                                   var = mean(v_ar))
    counter <- counter + 1
  }
}

results$n = factor(results$n)
results$p = factor(results$p)

ggplot(results, aes(x = n, y = mu, group=p, color=p)) +
  geom_line() +
  ggtitle(TeX('Consistency of estimator $\\mu_{\\hat{p}}$, average of 10 simulations')) +
  xlab("Number of possible edges") +
  ylab(TeX('$\\left|p_{analytical} - \\mu_{\\hat{p}}\\right|$')) +
  scale_color_discrete(name=TeX("$p_{analytical}$"))

ggplot(results, aes(x = n, y = var, group=p, color=p)) +
  geom_line() +
  ggtitle(TeX('Consistency of estimator $\\sigma^2_{\\hat{p}}$, average of 10 simulations')) +
  xlab("Number of possible edges") +
  ylab(TeX('$\\left|Var(p_{analytical}) - \\sigma^2_{\\hat{p}}\\right|$')) +
  scale_color_discrete(name=TeX("$p_{analytical}$"))
```

As we can see, as our number of possible edges increases, our estimators for $\mu$ and $\sigma^2$ converge, indicating we have consistent estimators. 

### Simulated Trials

In this experiment, we will analyze the power of our test developed. Assuming that the entire graph has average $p=0.5$, we will simulated from a block model where the probabiliy of the within-group edges have $p_{within}=0.5 + \epsilon$, and the outside of group edges have $p_{outside} = 0.5 - \epsilon$. We will assume a significance level of $0.95$ for our $T$ cutoff, and fix the number of observations between 0 and $\frac{2550}{2}=1225$, since our real data has $2450$ total edges yielding $1225$ observations per-group. Our simulation will be structured as follows:

+ Simulate $n$ graphs from a binomial distribution given $ne, p + \epsilon$, the alternative samples.
+ Simulate $n$ graphs from a binomial distribution given $ne, p - \epsilon$, the null samples.
+ Compute the empirical distribution for $\hat{p}$ for the alternative and null samples, respectively by repeating the above procedure $ns$ times.
+ derive the power from the respective empirical distribution of $\hat{p}$ as the fraction of test statistics more extreme than the critical test statistic.
+ compute the difference between the average simulated test statistic and the analytical test statistic.

```{r}
ana.welch_ttest = function(u1, u2, ne1, ne2, ns1=NaN, ns2=NaN, df=NaN, verbose=TRUE) {
  s1 = sqrt(model.var(p=u1, n=ne1))
  s2 = sqrt(model.var(p=u2, n=ne2))
  tstat = (u1 - u2)/sqrt(s1^2/ns1 + s2^2/ns2)
  if (!is.nan(df)) {
    df = df
  } else {
    dfnum = (s1^2/ns1 + s2^2/ns2)^2
    dfdenom = s1^4/(ns1^2*(ns1 - 1)) + s2^4/(ns2^2*(ns2-1))
    df = round(dfnum/dfdenom)
  }
  p = 1 - pt(tstat, df=df)
  return(list(t=tstat, p=p, df=df))
}

t.power = function(means, ne=1225, sig=.95, nsim=100, ngr=100) {
  ucut = qt(sig, df=ngr)  # t-statistic of null at the given significance level with ne-2 degrees of freedom
  ts = replicate(nsim, {  # replicate our described test n tsim times
    alt = replicate(ngr, sum(rbinom(n = ne, size=1, prob = means[1]))/ne)
    null = replicate(ngr, sum(rbinom(n = ne, size=1, prob = means[2]))/ne)
    t.test(alt, null, alternative = "greater", var.equal = FALSE)$statistic
  })
  ana_tstat = ana.welch_ttest(means[1], means[2], ne, ne, ngr, ngr)$t
  return(list(power=sum(ts > ucut)/nsim, diff=abs(mean(ts) - ana_tstat)/ana_tstat))
}

p = 0.5
diff = seq(0,  0.1, length=21)
ns = round(10^seq(1, log10(1225), length=10))
ndat = length(ns)*length(diff)
empty_ar = array(NaN, dim=c(ndat))
dat = data.frame(ns = empty_ar, diff=empty_ar, pow=empty_ar, tdiff=empty_ar)
counter = 1
for (j in 1:length(ns)) {
  n = ns[j]
  for (i in 1:length(diff)) {
    in.p = p + diff[i]/2
    out.p = p - diff[i]/2
    # under the model, assume the p_in is the mean within group, and p_out is the mean outside of group
    # compute the standard deviation according to the model
    means = c(in.p, out.p)
    result = t.power(means, ne=n)
    dat[counter,] = c(ns=n, diff=diff[i], pow=result$power, tdiff=result$diff)
    counter = counter + 1
  }
}
```

First, we look at power as a function of the number of edges in our simulation, as we vary the difference between the within community and outside community probabilities:

```{r}
dat$ns = factor(dat$ns)
dat$diff = factor(dat$diff)
thresh = data.frame(diff=diff, sig=.05)
thresh$diff = factor(thresh$diff)
ggplot(dat,  aes(x = diff, y = pow, group=ns, color=ns)) +
  geom_line() +
  ggtitle(TeX('Power of Unequal-Variance T-Test with 100 simulations, 100 $\\frac{graphs}{simulation}$')) +
  xlab(TeX('Difference in $\\left|p_{within} - p_{outside}\\right|$')) +
  ylab('Power of Test') +
  scale_color_discrete(name="number of edges") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

And we also look at how the analytical test-statistic computed from our trials compares to the empirical test-statistics estimated from our simulation procedure:

```{r}
ggplot(dat, aes(x = diff, y = tdiff, group=ns, color=ns)) +
  geom_line() +
  ggtitle(TeX('Analytical T-Test compared to Empirical T-Test')) +
  xlab(TeX('Difference in $\\left|p_{within} - p_{outside}\\right|$')) +
  ylab(TeX('$\\frac{\\left|\\bar{T}_{empirical} - T_{analytical}\\right|}{T_{analytical}}')) +
  scale_color_discrete(name="number of edges") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

# Real Data Experiments

## Raw Data

For the data, we compute the weighted mean functional (rank of each edge) and diffusion (number of fibers). For the functional connectome, we threshold such that the largest 50% of edges are set to connected, and the smallest 50% set to disconnected. For the diffusion (which are natively sparse) we just threshold edges that are present to connected, and edges that are not present to disconnected (threshold about 0). 

```{r}
require(igraph)
require(fmriutils)

thresh_matrix = function(matrix, thresh=0.5) {
  thr = quantile(matrix, thresh)
  return(ifelse(matrix > thr, 1, 0))
}

basepath = '/data/connectome_stats/'
fmri_gr = read_graph(file.path(basepath, 'fmrimean_1709.edgelist'), format="ncol")
fmri_mean = get.adjacency(fmri_gr, type="both", sparse=FALSE, attr='weight')
dwi_gr = read_graph(file.path(basepath, 'dwimean_2861.edgelist'), format="ncol")
dwi_mean = get.adjacency(dwi_gr, type="both", sparse=FALSE, attr='weight')

fmri_thresh = thresh_matrix(fmri_mean)
dwi_thresh = thresh_matrix(dwi_mean, thresh=0)

fmriu.plot.plot_graph(fmri_thresh, include_diag = TRUE, title = "Mean Thresholded Functional Connectome", legend.name = "connection")
fmriu.plot.plot_graph(dwi_thresh, include_diag = TRUE, title = "Mean Thresholded DWI Connectome", legend.name = "connection")
```

## Blocked Data

here, we will compute the probability of an edge existing in each of 4 quadrants (2 ipsilateral quadrants; 2 contralateral quadrants):

```{r}
block_data = function(matrix, groups) {
  # matrix is adwi_thresh n x n array
  # groups is a grouping of the vertices in the matrix as a list
  blocks = array(NaN, dim=c(2,2))
  for (i in 1:length(groups)) {
    for (j in 1:length(groups)) {
      blocks[i, j] = mean(matrix[groups[[i]], groups[[j]]])
    }
  }
  return(blocks)
}

group1 = 1:35
group2 = 36:70
groups = list(group1, group2)
fmri_block = block_data(fmri_thresh, groups)
dwi_block = block_data(dwi_thresh, groups)

fmriu.plot.plot_graph(fmri_block, title = "Blocked Functional Connectome", xlabel = "Hemisphere",
                      ylabel="Hemisphere", include_diag = TRUE, legend.name = "p")
fmriu.plot.plot_graph(dwi_block, title = "Blocked DWI Connectome", xlabel = "Hemisphere",
                      ylabel="Hemisphere", include_diag = TRUE, legend.name = "p")
```

## Difference in Ipsilateral vs. Contralateral Connectivity

### Per-Connectome

Here, we take each functional and diffusion connectome individually, and compute the parameters of our block model for each connectome. We collect all of the ipsi-lateral probabilities as our first sample, and the contra-lateral probabilities as our second sample:

#### Diffusion

```{r}
dsets = c('BNU1', 'BNU3', 'HNU1', 'KKI2009', 'NKI1', 'NKIENH', 'MRN1313', 'Templeton114', 'Templeton255', 'SWU4')
atlas = 'desikan'
basepath = '/data/dwi/edgelists'

graphobj = fmriu.io.collection.open_graphs(basepath = basepath, atlases = atlas, datasets = dsets, gname = 'ndmg_0-0-48/graphs', fmt='edgelist', rtype = 'array')
graphs = graphobj$graphs
datasets = graphobj$dataset
```

```{r}
ips.phat = array(NaN, dim=c(dim(graphs)[1]))
contr.phat = array(NaN, dim=c(dim(graphs)[1]))
for (i in 1:dim(graphs)[1]) {
  gr = thresh_matrix(graphs[i,,])
  ips = c(gr[group1, group1], gr[group2, group2])
  contr = c(gr[group1, group2], gr[group2, group1])
  ips.phat[i] = mean(ips)
  contr.phat[i] = mean(contr)
}

t.test(ips.phat, contr.phat, alternative="greater", var.equal=FALSE)
```

which as we can see, indicates a significant difference in ipsi-lateral connectivity compared to contra-lateral connectivity with $p < 2.2\times 10^{-16}$. 

#### Functional

```{r}
dsets = c('BNU1', 'BNU2', 'BNU3', 'HNU1', 'IBATRT', 'IPCAS1', 'IPCAS2', 'IPCAS5', 'IPCAS6', 'IPCAS8', 'MRN', 'NYU1', 'SWU1', 'SWU2', 'SWU3', 'SWU4', 'UWM', 'XHCUMS')
atlas = 'desikan-2mm'
basepath = '/data/fmri_edgelists'

graphobj = fmriu.io.collection.open_graphs(basepath = basepath, atlases = atlas, datasets=dsets, fmt='edgelist', rtype = 'array')
graphs = graphobj$graphs
datasets = graphobj$dataset
```

```{r}
ips.phat = array(NaN, dim=c(dim(graphs)[1]))
contr.phat = array(NaN, dim=c(dim(graphs)[1]))
for (i in 1:dim(graphs)[1]) {
  gr = thresh_matrix(graphs[i,,])
  ips = c(gr[group1, group1], gr[group2, group2])
  contr = c(gr[group1, group2], gr[group2, group1])
  ips.phat[i] = mean(ips)
  contr.phat[i] = mean(contr)
}

t.test(ips.phat, contr.phat, alternative="greater", var.equal=FALSE)
```
similar to the diffusion connectomes, the functional connectomes again exhibit a difference in intr-lateral vs. contra-lateral connectivity that is significant with $p < 2.2\times 10^{-16}$. 
  
### Aggregated 

Here, we aggregate all of the edge weights ipsilaterally and contralaterally, along with the number of edges this information is aggregated over for each connection type. We feed this into a simple t-test with the appropriate assumptions (unequal variance, goal is to test for ipsilateral connectivity exceeding contralateral connectivity):

### Functional

```{r}
fips = c(fmri_thresh[group1, group1], fmri_thresh[group2, group2])
fcontr = c(fmri_thresh[group1, group2], fmri_thresh[group2, group1])
ips.p = mean(fips)
contr.p = mean(fcontr)

ana.welch_ttest(ips.p, contr.p, length(fips), length(fcontr), ns1=1, ns2=1, df = 1)
```

### Diffusion

```{r}
dips = c(dwi_thresh[group1, group1], dwi_thresh[group2, group2])
dcontr = c(dwi_thresh[group1, group2], dwi_thresh[group2, group1])

ips.p = mean(dips)
contr.p = mean(dcontr)

ana.welch_ttest(ips.p, contr.p, length(dips), length(dcontr), ns1=1, ns2=1, df = 1)
```

As we can see above, the diffusion connectome is significant with $p=.015$, whereas the functional connectome is significant with just $p=.057$. Note that for this test, we only have one observation of each $\hat{p}$, so we use a t-test but for the degrees of freedom to $1$ (since it would otherwise be 0). 