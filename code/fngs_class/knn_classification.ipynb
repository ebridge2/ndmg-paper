{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NearestNeighbour Sex Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/scikit_learn-0.18.2-py2.7-linux-x86_64.egg/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cross_validation import LeaveOneOut, LeaveOneLabelOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from scipy.linalg import svd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 18}\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "np.random.seed(12345678)  # for reproducibility, set random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting File Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: BNU1 (108), BNU2 (121), BNU3 (48), SWU1 (60), SWU2 (54), SWU3 (46), HNU1 (300), IBATRT (50), IPCAS1 (60), IPCAS2 (66), IPCAS6 (18), IPCAS8 (26), LMU3 (50), MRN (90), NYU1 (75), UWM (50), XHCUMS (115), IACAS (52), SWU4 (466)\n",
      "Total Subjects: 1855\n"
     ]
    }
   ],
   "source": [
    "# Initializing dataset names\n",
    "dataset_names = dsets = ['BNU1', 'BNU2', 'BNU3', 'SWU1', 'SWU2', 'SWU3',\n",
    "        'HNU1', 'IBATRT', 'IPCAS1', 'IPCAS2',\n",
    "        'IPCAS6', 'IPCAS8', 'LMU3', 'MRN', 'NYU1', 'UWM', 'XHCUMS',\n",
    "        'IACAS', 'SWU4']\n",
    "# only these dsets bc of available covariates\n",
    "\n",
    "basepath = '/data/graphs/'\n",
    "pdata_path = '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/'\n",
    "# atlas = 'desikan'\n",
    "dir_names = [basepath + '/' + d for d in dataset_names]\n",
    "\n",
    "#  Crawls directories and creates a dictionary entry of file names for each\n",
    "#  dataset which we plan to process.\n",
    "fs = OrderedDict()\n",
    "for idx, dd in enumerate(dataset_names):\n",
    "    fs[dd] = [root + \"/\" + fl for root, dirs, files in os.walk(dir_names[idx])\n",
    "              for fl in files if fl.endswith(\".gpickle\")]\n",
    "\n",
    "ps = {dset: os.path.join(pdata_path, \"_\".join([dset, \"phenotypic_data.csv\"])) for dset in dsets}\n",
    "ps['MRN'] = '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/MRN1_phenotypic_data.csv'\n",
    "\n",
    "print \"Datasets: \" + \", \".join([fkey + ' (' + str(len(fs[fkey])) + ')'\n",
    "                                for fkey in fs])\n",
    "print \"Total Subjects: %d\" % (sum([len(fs[key]) for key in fs]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BNU1': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/BNU1_phenotypic_data.csv',\n",
       " 'BNU2': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/BNU2_phenotypic_data.csv',\n",
       " 'BNU3': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/BNU3_phenotypic_data.csv',\n",
       " 'HNU1': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/HNU1_phenotypic_data.csv',\n",
       " 'IACAS': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/IACAS_phenotypic_data.csv',\n",
       " 'IBATRT': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/IBATRT_phenotypic_data.csv',\n",
       " 'IPCAS1': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/IPCAS1_phenotypic_data.csv',\n",
       " 'IPCAS2': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/IPCAS2_phenotypic_data.csv',\n",
       " 'IPCAS6': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/IPCAS6_phenotypic_data.csv',\n",
       " 'IPCAS8': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/IPCAS8_phenotypic_data.csv',\n",
       " 'LMU3': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/LMU3_phenotypic_data.csv',\n",
       " 'MRN': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/MRN1_phenotypic_data.csv',\n",
       " 'NYU1': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/NYU1_phenotypic_data.csv',\n",
       " 'SWU1': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/SWU1_phenotypic_data.csv',\n",
       " 'SWU2': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/SWU2_phenotypic_data.csv',\n",
       " 'SWU3': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/SWU3_phenotypic_data.csv',\n",
       " 'SWU4': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/SWU4_phenotypic_data.csv',\n",
       " 'UWM': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/UWM_phenotypic_data.csv',\n",
       " 'XHCUMS': '/home/eric/Documents/research/ndmg-repos/ndmg-paper/code/fngs_class/XHCUMS_phenotypic_data.csv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/graphs//BNU1/connectomes/desikan-2mm/sub-0025867_ses-1_bold_desikan-2mm.gpickle'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs['BNU1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring Graph Read Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadGraphs(filenames, verb=False):\n",
    "    \"\"\"\n",
    "    Given a list of files, returns a dictionary of graphs\n",
    "\n",
    "    Required parameters:\n",
    "        filenames:\n",
    "            - List of filenames for graphs\n",
    "    Optional parameters:\n",
    "        verb:\n",
    "            - Toggles verbose output statements\n",
    "    \"\"\"\n",
    "    #  Initializes empty dictionary\n",
    "    gstruct = OrderedDict()\n",
    "    for idx, files in enumerate(filenames):\n",
    "        if verb:\n",
    "            print \"Loading: \" + files\n",
    "        #  Adds graphs to dictionary with key being filename\n",
    "        fname = os.path.basename(files)\n",
    "        gstruct[fname] = nx.read_gpickle(files)\n",
    "    return gstruct\n",
    "\n",
    "def constructGraphDict(names, fs, verb=False):\n",
    "    \"\"\"\n",
    "    Given a set of files and a directory to put things, loads graphs.\n",
    "\n",
    "    Required parameters:\n",
    "        names:\n",
    "            - List of names of the datasets\n",
    "        fs:\n",
    "            - Dictionary of lists of files in each dataset\n",
    "    Optional parameters:\n",
    "        verb:\n",
    "            - Toggles verbose output statements\n",
    "    \"\"\"\n",
    "    #  Loads graphs into memory for all datasets\n",
    "    graphs = OrderedDict()\n",
    "    for idx, name in enumerate(names):\n",
    "        if verb:\n",
    "            print \"Loading Dataset: \" + name\n",
    "        # The key for the dictionary of graphs is the dataset name\n",
    "        graphs[name] = loadGraphs(fs[name], verb=verb)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = constructGraphDict(dataset_names, fs, verb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNU1\n",
      "BNU2\n",
      "BNU3\n",
      "SWU1\n",
      "SWU2\n",
      "SWU3\n",
      "HNU1\n",
      "IBATRT\n",
      "IPCAS1\n",
      "IPCAS2\n",
      "IPCAS6\n",
      "IPCAS8\n",
      "LMU3\n",
      "MRN\n",
      "NYU1\n",
      "UWM\n",
      "XHCUMS\n",
      "IACAS\n",
      "SWU4\n"
     ]
    }
   ],
   "source": [
    "phenotypes = OrderedDict()\n",
    "for dataset in dataset_names:\n",
    "    print(dataset)\n",
    "    tmp = csv.reader(open(ps[dataset], 'rU'))\n",
    "    pheno = OrderedDict()\n",
    "    if dataset == 'KKI2009':\n",
    "        triple = [[t[1].strip(), t[4], int(t[5] == 'M')] for t in tmp][1:]  # female=F->0, male=M->1\n",
    "    elif dataset == 'Templeton114':\n",
    "        triple = [[t[0].lstrip('0'), t[3], int(t[2] == '1')] for t in tmp][1:]  # female=0->0, male=1->1\n",
    "    else:\n",
    "        triple = [[t[0].lstrip('0'), t[2], int(t[3] == '2')] for t in tmp\n",
    "                  if t[3] != '#' and t[2] != '#'][1:]  # female=1->0, male=2->1\n",
    "    \n",
    "    for idx, trip in enumerate(triple):\n",
    "        pheno[trip[0]] = trip[1:]\n",
    "    phenotypes[dataset] = pheno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Graphs to Vectors and Phenotypes to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNU1\n",
      "BNU2\n",
      "BNU3\n",
      "SWU1\n",
      "SWU2\n",
      "SWU3\n",
      "HNU1\n",
      "IBATRT\n",
      "IPCAS1\n",
      "IPCAS2\n",
      "IPCAS6\n",
      "IPCAS8\n",
      "LMU3\n",
      "MRN\n",
      "NYU1\n",
      "UWM\n",
      "XHCUMS\n",
      "IACAS\n",
      "SWU4\n",
      "(1855, 2415) (1855,) (1855,) 1855\n",
      "955 900\n"
     ]
    }
   ],
   "source": [
    "N = nx.number_of_nodes(graphs[graphs.keys()[0]][graphs[graphs.keys()[0]].keys()[0]])\n",
    "feat = np.empty((0, int(sp.special.binom(N,2))), int)\n",
    "dats = list(())\n",
    "ages = np.array(())\n",
    "sexy = np.array(())\n",
    "sbjs = list(())\n",
    "\n",
    "for idx1, dset in enumerate(graphs):\n",
    "    print(dset)\n",
    "    for idx2, subj in enumerate(graphs[dset]):\n",
    "        A = nx.adjacency_matrix(graphs[dset][subj]).todense()\n",
    "        Au = A[np.triu_indices(A.shape[0], 1)]\n",
    "        feat = np.append(feat, Au, axis=0)\n",
    "        dats.append(dset)\n",
    "        try:\n",
    "            subj_id = str(int(subj.split('-')[1].split('_')[0]))\n",
    "        except:\n",
    "            subj_id = subj.split('-')[1].split('_')[0]\n",
    "\n",
    "        sbjs.append(subj_id)\n",
    "        \n",
    "        try:\n",
    "            ages = np.append(ages, int(float(phenotypes[dset][subj_id.lstrip('0')][0])))\n",
    "            sexy = np.append(sexy, int(phenotypes[dset][subj_id.lstrip('0')][1]))\n",
    "        except Exception as e:\n",
    "            print(subj_id)\n",
    "print feat.shape, ages.shape, sexy.shape, len(sbjs)\n",
    "print sum(sexy == 0), sum(sexy == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1845, 2415) (1845,) (1845,) 1845\n"
     ]
    }
   ],
   "source": [
    "# delete matrices w zero entries\n",
    "incl = np.logical_not(np.isnan(np.sum(feat, axis=1)))\n",
    "# incl = incl.squeeze\n",
    "# for some reason squeeze above doesn't work..\n",
    "incl_ar = np.empty(incl.shape[0], dtype=bool)\n",
    "for i in range(0, incl.shape[0]):\n",
    "    incl_ar[i] = incl[i,0]\n",
    "ages = ages[incl_ar]\n",
    "sexy = sexy[incl_ar]\n",
    "sbjs = np.array(sbjs)[incl_ar]\n",
    "feat = feat[incl_ar, :]\n",
    "print feat.shape, ages.shape, sexy.shape, len(sbjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "---------\n",
    "\n",
    "## Classify Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN LOO Scan Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbourhoods complete: 3 5 7 9 11 13 15 17"
     ]
    }
   ],
   "source": [
    "c_mean_loo_scan = np.array(())\n",
    "c_std_loo_scan = np.array(())\n",
    "neighbourhoods = (np.arange(19)+2)*2-1\n",
    "print \"Neighbourhoods complete:\",\n",
    "for i in neighbourhoods:\n",
    "    classif = KNeighborsClassifier(i)\n",
    "    loo = LeaveOneOut(len(sexy))\n",
    "    score = cross_validation.cross_val_score(classif, feat, sexy, cv=loo)\n",
    "    c_mean_loo_scan = np.append(c_mean_loo_scan, score.mean())\n",
    "    c_std_loo_scan = np.append(c_std_loo_scan, score.std())\n",
    "    print i,\n",
    "print \".\"\n",
    "#     print(\"Accuracy for community size %d: %0.2f (+/- %0.2f)\" % (i, score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN LOO Subject Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mean_loo_subj = np.array(())\n",
    "c_std_loo_subj = np.array(())\n",
    "print \"Neighbourhoods complete:\",\n",
    "for i in neighbourhoods:\n",
    "    classif = KNeighborsClassifier(i)\n",
    "    lolo = LeaveOneLabelOut(sbjs)\n",
    "    score = cross_validation.cross_val_score(classif, feat, sexy, cv=lolo)\n",
    "    c_mean_loo_subj = np.append(c_mean_loo_subj, score.mean())\n",
    "    c_std_loo_subj = np.append(c_std_loo_subj, score.std())\n",
    "    print i,\n",
    "print \".\"\n",
    "#     print(\"Accuracy for community size %d: %0.2f (+/- %0.2f)\" % (i, score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN LOO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = OrderedDict()\n",
    "for i in neighbourhoods:\n",
    "    classif = KNeighborsClassifier(i)\n",
    "    tmp = np.array(())\n",
    "    for idx, dset in enumerate(dataset_names):\n",
    "        ind = np.array([dset == d for d in dats])\n",
    "        ft_train = feat[ind < 1] # current dataset is test\n",
    "        ft_test = feat[ind >= 1] # all others are train\n",
    "        sx_train = sexy[ind < 1]\n",
    "        sx_test = sexy[ind >= 1]\n",
    "        classif.fit(ft_train, sx_train)\n",
    "        score = classif.score(ft_test, sx_test)\n",
    "        tmp = np.append(tmp, score)\n",
    "#         print \"Classification Accuracy: %.5f (%0.5f)\" % (score, 1-score)\n",
    "    knn[i] = tmp\n",
    "\n",
    "c_knn_loo_dsetx = np.array(())\n",
    "c_knn_loo_dsety = np.array(())\n",
    "for i in neighbourhoods:\n",
    "    c_knn_loo_dsetx = np.append(c_knn_loo_dsetx, np.repeat(i, len(dataset_names))+1*np.random.rand(len(dataset_names)))\n",
    "    c_knn_loo_dsety = np.append(c_knn_loo_dsety, knn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN LOO Dataset after subtracting cohort mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_cm = np.copy(feat)\n",
    "# print feat[600]\n",
    "for idx, dset in enumerate(dataset_names):\n",
    "    ind = np.array([dset == d for d in dats])\n",
    "    curr = feat_cm[ind < 1] # current dataset is test\n",
    "    mean = np.mean(curr, axis=0)\n",
    "    updated = curr - mean\n",
    "    feat_cm[ind < 1] = updated\n",
    "# print feat_cm[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = OrderedDict()\n",
    "for i in neighbourhoods:\n",
    "    classif = KNeighborsClassifier(n_neighbors=2*(i+1)-1)\n",
    "    tmp = np.array(())\n",
    "    for idx, dset in enumerate(dataset_names):\n",
    "        ind = np.array([dset == d for d in dats])\n",
    "        ft_train = feat_cm[ind < 1] # current dataset is test\n",
    "        ft_test = feat_cm[ind >= 1] # all others are train\n",
    "        sx_train = sexy[ind < 1]\n",
    "        sx_test = sexy[ind >= 1]\n",
    "        classif.fit(ft_train, sx_train)\n",
    "        score = classif.score(ft_test, sx_test)\n",
    "        tmp = np.append(tmp, score)\n",
    "#         print \"Classification Accuracy: %.5f (%0.5f)\" % (score, 1-score)\n",
    "    knn[i] = tmp\n",
    "\n",
    "c_knn_loo_dset_cmx = np.array(())\n",
    "c_knn_loo_dset_cmy = np.array(())\n",
    "for i in neighbourhoods:\n",
    "    c_knn_loo_dset_cmx = np.append(c_knn_loo_dset_cmx, np.repeat(i, len(dataset_names))+1*np.random.rand(len(dataset_names)))\n",
    "    c_knn_loo_dset_cmy = np.append(c_knn_loo_dset_cmy, knn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN LOO Dataset after subtracting population and then cohort means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm = np.mean(feat, axis=0)\n",
    "feat_pm = feat - pm\n",
    "\n",
    "feat_pm_cm = np.copy(feat_pm)\n",
    "# print feat_pm[600]\n",
    "for idx, dset in enumerate(dataset_names):\n",
    "    ind = np.array([dset == d for d in dats])\n",
    "    curr = feat_pm_cm[ind < 1] # current dataset is test\n",
    "    mean = np.mean(curr, axis=0)\n",
    "    updated = curr - mean\n",
    "    feat_pm_cm[ind < 1] = updated\n",
    "# print feat_pm_cm[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = OrderedDict()\n",
    "for i in neighbourhoods:\n",
    "    classif = KNeighborsClassifier(n_neighbors=2*(i+1)-1)\n",
    "    tmp = np.array(())\n",
    "    for idx, dset in enumerate(dataset_names):\n",
    "        ind = np.array([dset == d for d in dats])\n",
    "        ft_train = feat_pm_cm[ind < 1] # current dataset is test\n",
    "        ft_test = feat_pm_cm[ind >= 1] # all others are train\n",
    "        sx_train = sexy[ind < 1]\n",
    "        sx_test = sexy[ind >= 1]\n",
    "        classif.fit(ft_train, sx_train)\n",
    "        score = classif.score(ft_test, sx_test)\n",
    "        tmp = np.append(tmp, score)\n",
    "#         print \"Classification Accuracy: %.5f (%0.5f)\" % (score, 1-score)\n",
    "    knn[i] = tmp\n",
    "\n",
    "c_knn_loo_dset_pmcmx = np.array(())\n",
    "c_knn_loo_dset_pmcmy = np.array(())\n",
    "for i in neighbourhoods:\n",
    "    c_knn_loo_dset_pmcmx = np.append(c_knn_loo_dset_pmcmx, np.repeat(i, len(dataset_names))+1*np.random.rand(len(dataset_names)))\n",
    "    c_knn_loo_dset_pmcmy = np.append(c_knn_loo_dset_pmcmy, knn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN LOO Dataset after normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_un = normalize(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = OrderedDict()\n",
    "for i in neighbourhoods:\n",
    "    classif = KNeighborsClassifier(n_neighbors=2*(i+1)-1)\n",
    "    tmp = np.array(())\n",
    "    for idx, dset in enumerate(dataset_names):\n",
    "        ind = np.array([dset == d for d in dats])\n",
    "        ft_train = feat_un[ind < 1] # current dataset is test\n",
    "        ft_test = feat_un[ind >= 1] # all others are train\n",
    "        sx_train = sexy[ind < 1]\n",
    "        sx_test = sexy[ind >= 1]\n",
    "        classif.fit(ft_train, sx_train)\n",
    "        score = classif.score(ft_test, sx_test)\n",
    "        tmp = np.append(tmp, score)\n",
    "#         print \"Classification Accuracy: %.5f\" % (score)\n",
    "    knn[i] = tmp\n",
    "\n",
    "c_knn_loo_dset_unx = np.array(())\n",
    "c_knn_loo_dset_uny = np.array(())\n",
    "for i in neighbourhoods:\n",
    "    c_knn_loo_dset_unx = np.append(c_knn_loo_dset_unx, np.repeat(i, len(dataset_names))+1*np.random.rand(len(dataset_names)))\n",
    "    c_knn_loo_dset_uny = np.append(c_knn_loo_dset_uny, knn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "---------\n",
    "\n",
    "## Save the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_file1 = basepath + '/classification_results.npz'\n",
    "np.savez(result_file1,\n",
    "         neighbourhoods=neighbourhoods,\n",
    "         c_mean_loo_scan=c_mean_loo_scan, c_std_loo_scan=c_std_loo_scan,\n",
    "         c_mean_loo_subj=c_mean_loo_subj, c_std_loo_subj=c_std_loo_subj,\n",
    "         c_knn_loo_dsetx=c_knn_loo_dsetx, c_knn_loo_dsety=c_knn_loo_dsety,\n",
    "         c_knn_loo_dset_cmx=c_knn_loo_dset_cmx, c_knn_loo_dset_cmy=c_knn_loo_dset_cmy,\n",
    "         c_knn_loo_dset_pmcmx=c_knn_loo_dset_pmcmx, c_knn_loo_dset_pmcmy=c_knn_loo_dset_pmcmy,\n",
    "         c_knn_loo_dset_unx=c_knn_loo_dset_unx, c_knn_loo_dset_uny=c_knn_loo_dset_uny)\n",
    "\n",
    "feature_file = basepath + '/classified_vectors.npz'\n",
    "np.savez(feature_file,\n",
    "         ages=ages,\n",
    "         sexy=sexy,\n",
    "         sbjs=sbjs,\n",
    "         feat=feat,\n",
    "         feat_cm=feat_cm,\n",
    "         feat_pm_cm=feat_pm_cm,\n",
    "         feat_un=feat_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "---------\n",
    "\n",
    "## Load the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_file = basepath + '/classified_vectors.npz'\n",
    "result_file1 = basepath + '/classification_results.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "dat = np.load(feature_file)\n",
    "dat.files\n",
    "d2 = np.load(result_file1)\n",
    "for key in d2:\n",
    "    exec('{KEY} = {VALUE}'.format(KEY = key, VALUE = repr(d2[key]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "--------\n",
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_knn_loo_dsetx = np.floor(c_knn_loo_dsetx)\n",
    "c_knn_loo_dset_cmx = np.floor(c_knn_loo_dset_cmx)\n",
    "c_knn_loo_dset_pmcmx= np.floor(c_knn_loo_dset_pmcmx)\n",
    "c_knn_loo_dset_unx = np.floor(c_knn_loo_dset_unx)\n",
    "\n",
    "c_mean_loo_dset = np.array(())\n",
    "c_std_loo_dset = np.array(())\n",
    "\n",
    "c_mean_loo_dset_cm = np.array(())\n",
    "c_std_loo_dset_cm = np.array(())\n",
    "\n",
    "c_mean_loo_dset_pmcm = np.array(())\n",
    "c_std_loo_dset_pmcm = np.array(())\n",
    "\n",
    "c_mean_loo_dset_un = np.array(())\n",
    "c_std_loo_dset_un = np.array(())\n",
    "\n",
    "for idx in neighbourhoods:\n",
    "    tmp = c_knn_loo_dsety[c_knn_loo_dsetx == idx]\n",
    "#     print len(tmp)\n",
    "    c_mean_loo_dset = np.append(c_mean_loo_dset, np.mean(tmp))\n",
    "    c_std_loo_dset = np.append(c_std_loo_dset, np.std(tmp))\n",
    "    \n",
    "    tmp_cm = c_knn_loo_dset_cmy[c_knn_loo_dset_cmx == idx]\n",
    "#     print len(tmp_cm)\n",
    "    c_mean_loo_dset_cm = np.append(c_mean_loo_dset_cm, np.mean(tmp_cm))\n",
    "    c_std_loo_dset_cm = np.append(c_std_loo_dset_cm, np.std(tmp_cm))\n",
    "\n",
    "    tmp_pmcm = c_knn_loo_dset_pmcmy[c_knn_loo_dset_pmcmx == idx]\n",
    "#     print len(tmp_pmcm)\n",
    "    c_mean_loo_dset_pmcm = np.append(c_mean_loo_dset_pmcm, np.mean(tmp_pmcm))\n",
    "    c_std_loo_dset_pmcm = np.append(c_std_loo_dset_pmcm, np.std(tmp_pmcm))\n",
    "    \n",
    "    tmp_pmcm = c_knn_loo_dset_uny[c_knn_loo_dset_unx == idx]\n",
    "#     print len(tmp_pmcm)\n",
    "    c_mean_loo_dset_un = np.append(c_mean_loo_dset_un, np.mean(tmp_pmcm))\n",
    "    c_std_loo_dset_un = np.append(c_std_loo_dset_un, np.std(tmp_pmcm))\n",
    "    \n",
    "print len(c_std_loo_dset_cm), len(c_mean_loo_dset_cm),\n",
    "print len(c_std_loo_dset_pmcm), len(c_mean_loo_dset_pmcm), len(c_mean_loo_dset_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorlover as cl\n",
    "from IPython.display import HTML\n",
    "\n",
    "types = ['Chance', 'LOO Session', 'LOO Subj', 'LOO Dataset',\n",
    "         'LOO Dataset (UN)', 'LOO Dataset (-CM)', 'LOO Dataset (-PM-CM)']\n",
    "\n",
    "op = 0.8\n",
    "\n",
    "cols = OrderedDict()\n",
    "cols[types[0]] = 'rgba(100,100,100, {})'.format(op)\n",
    "cols[types[1]] = 'rgba(0, 170, 130, {})'.format(op)\n",
    "cols[types[2]] = 'rgba(97, 204, 0, {})'.format(op)\n",
    "cols[types[3]] = 'rgba(190, 250, 0, {})'.format(op)\n",
    "cols[types[4]] = 'rgba(255, 158, 0, {})'.format(op)\n",
    "cols[types[5]] = 'rgba(255, 95, 0, {})'.format(op)\n",
    "cols[types[6]] = 'rgba(171, 64, 0, {})'.format(op)\n",
    "\n",
    "# for ty in types:\n",
    "#     cols[ty] = ....\n",
    "\n",
    "# cols_list = cl.to_numeric(cols.values())\n",
    "# cols_list = [tuple([cpit/255 for cpit in cthing]) for cthing in cols_list]\n",
    "HTML(cl.to_html(cols.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndmg.stats.plotly_helper as plh\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot, plot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode()\n",
    "\n",
    "# Building plotting dictionary\n",
    "classif = OrderedDict()\n",
    "chance = np.max((sum(sexy), len(sexy)-sum(sexy)))/len(sexy)\n",
    "classif[types[0]] = {'x': [0, np.max(neighbourhoods)+2], 'y':[chance, chance], 'err': [0, 0]}\n",
    "classif[types[1]] = {'x': list(neighbourhoods-0.5), 'y': list(c_mean_loo_scan),      'err': list(c_std_loo_scan/np.sqrt(len(sexy)))}\n",
    "classif[types[2]] = {'x': list(neighbourhoods-0.3), 'y': list(c_mean_loo_subj),      'err': list(c_std_loo_subj/np.sqrt(len(sexy)))}\n",
    "classif[types[3]] = {'x': list(neighbourhoods-0.1), 'y': list(c_mean_loo_dset),      'err': list(c_std_loo_dset/np.sqrt(len(dataset)))}\n",
    "classif[types[4]] = {'x': list(neighbourhoods+0.1), 'y': list(c_mean_loo_dset_un),   'err': list(c_std_loo_dset_un/np.sqrt(len(dataset)))}\n",
    "classif[types[5]] = {'x': list(neighbourhoods+0.3), 'y': list(c_mean_loo_dset_cm),   'err': list(c_std_loo_dset_cm/np.sqrt(len(dataset)))}\n",
    "classif[types[6]] = {'x': list(neighbourhoods+0.5), 'y': list(c_mean_loo_dset_pmcm), 'err': list(c_std_loo_dset_pmcm/np.sqrt(len(dataset)))}\n",
    "\n",
    "data = []\n",
    "for cl in classif.keys()[::-1]:\n",
    "    vis = False if cl is 'Chance' else True\n",
    "    das = 'dash' if cl is 'Chance' else 'line'\n",
    "    shape = 'none' if cl is 'Chance' else 'dot'\n",
    "    data += [\n",
    "        Scatter(x=classif[cl]['x'],\n",
    "                y=classif[cl]['y'],\n",
    "                error_y=dict(\n",
    "                    type = 'data',\n",
    "                    array = classif[cl]['err'],\n",
    "                    visible = vis,\n",
    "                    color=cols[cl],\n",
    "                ),\n",
    "                marker=Marker(\n",
    "                    symbol=shape\n",
    "                ),\n",
    "                line=Line(\n",
    "                    color=cols[cl],\n",
    "                    width=3,\n",
    "                    dash = das,\n",
    "                ),\n",
    "                name=cl,\n",
    "                legendgroup=cl\n",
    "        )\n",
    "    ]\n",
    "\n",
    "fig = Figure(data=data)\n",
    "fig.layout['yaxis']['range'] = [0, 1]\n",
    "fig.layout['yaxis']['title'] = 'Classification Accuracy'\n",
    "fig.layout['yaxis']['nticks'] = 3\n",
    "\n",
    "fig.layout['xaxis']['range'] = [2, np.max(neighbourhoods)+1]\n",
    "fig.layout['xaxis']['title'] = 'Neighbourhood Size (k)'\n",
    "fig.layout['xaxis']['nticks'] = 3\n",
    "\n",
    "# fig.layout['legend']['orientation'] = 'h'\n",
    "\n",
    "fig.layout['title'] = 'KNN Sex Classification Over Multiple Datasets'\n",
    "iplot(fig, validate=False, filename='knn-sex-classification.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(1.2*8,8))\n",
    "plt.errorbar(neighbourhoods-0.9, c_mean_loo_scan, c_std_loo_scan/np.sqrt(len(sexy)), lw=3, color=cols_list[1]) #'#00ee00')\n",
    "plt.errorbar(neighbourhoods-0.6, c_mean_loo_subj, c_std_loo_subj/np.sqrt(len(sexy)), lw=3, color=cols_list[2]) #'#ee0000')\n",
    "plt.hold(True)\n",
    "plt.axhline(sum(sexy)/len(sexy), linestyle='--', lw=5, alpha=0.5, color=cols_list[0]) #\"#0000ee\")\n",
    "plt.errorbar(neighbourhoods-0.3, c_mean_loo_dset, c_std_loo_dset/np.sqrt(len(dataset*1)), lw=3, color=cols_list[3]) #'#000000')\n",
    "plt.errorbar(neighbourhoods+0.3, c_mean_loo_dset_cm, c_std_loo_dset_cm/np.sqrt(len(dataset*1)), lw=3, color=cols_list[4]) #'#ee00ee')\n",
    "plt.errorbar(neighbourhoods+0.6, c_mean_loo_dset_pmcm, c_std_loo_dset_pmcm/np.sqrt(len(dataset*1)), lw=3, color=cols_list[5]) #'#eeee00')\n",
    "plt.errorbar(neighbourhoods+0.9, c_mean_loo_dset_un, c_std_loo_dset_un/np.sqrt(len(dataset*1)), lw=3, color=cols_list[6]) #'#606060')\n",
    "\n",
    "# plt.ylim((0.5, 0.7))\n",
    "# plt.yticks((0.5, 0.6, 0.7),(0.5, 0.6, 0.7))\n",
    "# plt.xticks((0, 20, 40),(0, 20, 40))\n",
    "plt.xlim([2, 40.3])\n",
    "plt.ylim([0.35, 0.8])\n",
    "plt.xlabel('Size of Neighbourhood (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Sex Classification With Multiple Datasets')\n",
    "lgd = plt.legend(types,\n",
    "                 fontsize=16, bbox_to_anchor = (1.5, 1))\n",
    "plt.tight_layout(1)\n",
    "plt.savefig(basepath + 'classification_desikan.png',\n",
    "            bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
